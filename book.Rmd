---
title: "Spatial microsimulation with R"
output:
  pdf_document:
    fig_caption: yes
    includes: null
    keep_tex: yes
    number_sections: yes
    toc: yes
bibliography: ~/Documents/smr.bib 
layout: default
---

```{r, echo=FALSE}
# This is an early draft of a book on spatial microsimulation, for teaching purposes
# Introduction {#Introduction}
```

# Introduction

Spatial microsimulation is a statistical technique for combining individual and
geographical data. The resulting *spatial microdata* are useful in many
situations where individual-level and geographically specific
processes are in operation, enabling modelling and analysis on multiple levels.
Spatial microsimulation can also be seen as an approach to better understand
the world. The term
is little known outside the fields of human geography and regional
science yet its methods have the potential to be useful in a wide
range of applications. Spatial microsimulation
has great potential for informing public policy for social
benefit in housing, transport and
sustainable urban planning to prepare for a post carbon world
--- after we stop burning fossil fuels.

There is growing interest in spatial microsimulation. This is due largely to its
practical utility in an era of 'evidence-based policy' but is also driven by
changes in the wider research environment inside and outside of academia.
Continued improvements in computers, software and data availability mean
spatial microsimulation is more accessible than ever. It is now possible to
simulate the populations of small administrative areas at the individual-level
almost anywhere in the world. This opens new possibilities for a range of
applications, not least policy evaluation.

Still, the meaning of spatial microsimulation is ambiguous for many. This is
partly because the technique is inherently difficult to understand and partly
due to researchers themselves: some uses of the term in the academic literature
are unclear or inconsistent about what the method entails.  Worse is work that
treats spatial microsimulation as a magical black box. This book is about
demystifying spatial microsimulation.

Spatial
microsimulation can be understood either as a technique or an approach:

1. A *technique* for generating spatial microdata --- individuals allocated to
zones (see Figure 1).
2. An *approach* to modelling based on
spatial microdata, simulated or real.

```{r, fig.cap="Schematic of the spatial microsimulation technique", fig.width=4, fig.height=4, echo=FALSE}
library(png)
library(grid)
img <- readPNG("figures/msim-schema-lowres.png")
grid.raster(img)
```

Throughout this book we will see spatial microsimulation in
both senses of the term, generally moving from the former
to the latter perspective as the chapters progress.

Another issue tackled in this book is reproducibility.  Most findings in the
field cannot easily be replicated, meaning there is no way of independently
checking the results.  In today’s age of fast Internet connections, open access
datasets and free software, there is little excuse for this.  The issue is not
unique to the field of
spatial microsimulation. Opaque methods, impossible to replicate,
are widespread in academia, leading to
calls for an 'Open Regional Science'
[@Rey2014].
Similar proposals made in other research areas are gaining
traction
[@Ince2012; @Peng2006; and most recently, @McNutt2014].
This book
encourages such a shift towards transparency
in the field of spatial microsimulation.

Reproducibility is encouraged throughout via provision of code for readers
to actually *do* spatial microsimulation. Small yet realistic datasets are
provided to run the methods on your own computer.  All the findings presented in this
book can therefore be reproduced using code and data in the book's Github
[repository](https://github.com/Robinlovelace/spatial-microsim-book).

Why spend time and effort on reproducibility? The first answer pragmatic:
reproducibility can actually save time in the long-run, by ensuring more readable
code; reproducibility can increase the re-usability and impact of
research, allowing methods to be re-run or modified at a later data.  The second
reason is more profound: reproducibility is a prerequisite of falsifiability and
falsifiability is the backbone of science [@Popper1959].  The results of
non-reproducible research cannot be verified, reducing scientific credibility.
These philosophical observations inform the book’s practical nature.

```{r, echo=FALSE}
# Why spend time and effort on reproducibility? The first answer is that
# reproducibility actually saves time in the long-run, by ensuring more
# readable code and allowing your results to be easily re-run at a later data.
# The second reason is more profound. Reproducibility is a prerequisite
# of falsifiability and falsifiability is the backbone of science
# (Popper, 1959).
# The results on non-reproducible research cannot be verified, reducing scientific
# credibility. These observations inform the book’s practical nature.
# The aim is simple: to provide a foundation in spatial microsimulation.
# http://en.wikipedia.org/wiki/Experiential_learning
# Poper's link also here
# [2011](http://www.manning.com/kabacoff/)
```

This book presents spatial microsimulation as a living, evolving set of
techniques rather than a prescriptive formula for arriving at the 'right'
answer.  Spatial microsimulation is largely defined by its user-community, made
up of a growing number of people worldwide. This book
aims to contribute to the community by encouraging collaboration, innovation and rigour. It also encourages playing with the methods and
'getting your hands dirty' with the code. As
@Kabakoff
put it regarding R, "the best way to learn is to
experiment". 

## Why spatial microsimulation with R? {#whyR}

```{r, echo=FALSE}
# @Kabakoff [p. xxii]
# The book aims to make spatial microsimulation accessible to more people,
# with a practical approach that encourages playing with the data and code.
# expressing oneself.^[This video introduces the idea of
# expressing oneself in [R](http://youtu.be/wki0BqlztCo)].
# [Hölm (1987, p.
# 153)](http://www.jstor.org/stable/10.2307/490448)
# @Holm1987 [p. 153]
```

Software decisions have a major impact on the flexibility, efficiency and
reproducibility of research.  Nearly three decades ago
@Holm1987
observed that "little
attention is paid to the choice of programming language used" for
microsimulation. This appears to be as true now as it was then. Software is
rarely discussed in papers on the subject and there are few mature spatial
microsimulation packages.^[The Flexible Modelling Framework
([FMF](https://github.com/MassAtLeeds/FMF)) is a notable exception written in
Java that can perform various modelling tasks.] 
Factors that should influence software selection including cost, maturity,
features and performance. Perhaps most important for busy researchers are the
ease and speed of learning,
writing, adapting and communicating the analysis. R excels in
each of these areas, hence the choice of R.

```{r, echo=FALSE}
# Yet the software used has a lasting
# impact, including what can and cannot be done
# and opportunities for collaboration.  explains the choice of R.
# In my own research, for example, a conscious decision was made early on to use
# R. This had subsequent knock-on impacts on
# the features, analysis and even design of my simulations.
# There are hundreds computer programming languages and many of these
# are general purpose and 'Turing complete', meaning they could, with
# sufficient effort, perform spatial microsimulation (or any other
# numerical operation). So why choose R?
# ^[Speed
# of execution is an arguable exception, an issue that can be tackled
# by vectorisation (see [Appendix A](#apR)) and judicious use of add-on *R packages*.]
```

R is a *low-level* language compared with statistical programs based on a strong
graphical user interface (GUI) such as Microsoft Excel and SPSS.  R offers great
flexibility for analysing and modelling data and enables easy creation of
user-defined functions. These are all desirable attributes of
software for undertaking spatial microsimulation.
On the other hand, R is *high-level* compared with
general purpose languages such as C and Python.  Instead of writing code to
perform statistical operations 'from scratch', R users generally use pre-made
functions. To calculate the mean value of variable `x`, for example, one would
need to type 20 characters in Python: `float(sum(x))/len(x)`.^[The `float`
function is needed in case whole numbers are used. This can be reduced to 13
characters with the excellent **NumPy** package: `import numpy; x = [1,3,9];
numpy.mean(x)` would generate the desired result. The R equivalent is `x =
c(1,3,9); mean(x)`.] In pure R just 7 characters are sufficient: `mean(x)`. This
terseness and range of pre-made functions is useful for ease of
reading and writing spatial microsimulation models and analysing the results.

```{r, echo=FALSE}
# One may argue that saving a few keystrokes while writing
# code is not a priority but it is certain
# that the time savings of being concise can be vast.
```

The example of calculating the mean in R and Python
may be trite but illustrates a wider point: R
was *designed* to work with statistical data, so many functions in the default R
installation (e.g. `lm()`, to create a linear regression model) perform
statistical analysis 'out of the box'.  In agent-based modelling, the
statistical analysis of results often occupies more time than running the model
itself
[@thiele2014r].
The same
applies to spatial microsimulation, making R an ideal choice due to its
statistical analysis capabilities.

Finally, R has an active and  growing user community. As a result there are
thousands of packages that extend R's capabilities by providing new functions.
Improvements are being added all the time.
The **ipfp** package, for example,
can greatly reduce the computational time taken for a key element of spatial
microsimulation process, as we shall see in Chapter 5 in
*Reweighting with ipfp* ([](#ipfp)).
Further information about why R is a good choice for spatial microsimulation
is provided in the Appendix, a tutorial introduction to R for spatial
microsimulation applications. The next section describes approaches
to learning R in general terms.

```{r, echo=FALSE}
# For speed-critical applications,
# R provides access to lower level languages. It
# is possible to say a lot in R in few lines of code,
# but it is also possible for users to create their own
# commands, allowing users complete control. 
# The reasons for using R for spatial
# microsimulation can be summarised by modifying
# the arguments put forward by Norman Matloff (2001)
# for using R in general. R is:
# 
# -  "the de facto standard among
#     professional statisticians", meaning that the spatial microsimulation
#     code can easily be modified to perform a variety of statistical operations.
#  
# 
# -   "a general
#     programming language, so that you can automate your analyses and
#     create new functions." This is particularly useful if you need to run the same
#     code in many different ways for many locations. In R, the computer
#     can be asked to iterate over as many combinations of model runs as desired.
# 
# -   open source, meaning its easy to share your code and reproduce your
#     findings anywhere in the world, without the worry of infringing copyright
#     licences. In work funded by the public, this also has a large benefit
#     in terms of education and the democratisation of research.
```

## Learning the R language {#learningR}

Having learned a little about *why* R is a good tool for the job, it is worth
considering at this stage *how* R should be used.  It is useful to think of R
not as a series of isolated commands, but as an interconnected *language*.
The code is used not only for the computer to crunch numbers,
but also to communicate ideas, from one person to another.
In other words, this book teaches spatial microsimulation in the language of R.
Of course, English is more appropriate than R for *explaining* rather than
merely describing the method and the language of mathematics is ideal
for describing quantitative relationships conceptually. However,
because the practical components of this book are implemented in R, you will gain
more from it if you are fluent in R. To this end the book aims to improve your
R skills as well as your ability to perform spatial microsimulation, especially
in the earlier practical chapters. Some prior knowledge of R will make
reading this book easier, but R novices should be able to follow the worked
examples, with reference to appropriate supplementary material
(see the references listed in the Appendix [](#further)). As
with learning Spanish or Chinese, frequent practice, persistence and
experimentation will ensure deep learning.

A more practical piece of advice is to organise your workflow.  Each
project should have its own self-contained folder containing all that is needed
to replicate the analysis, except perhaps large input datasets. This could
include the raw (unchanged) input data^[Raw data should be kept safely on an
external hard disk or a server if it is large or sensitive.], R code for analysis,
the graphical outputs and files containing data outputs.  To avoid clutter,
it is sensible to arrange this content into folders, as illustrated below
(thanks to Colin Gillespie for this tip): 

```
|-- book.Rmd
|-- data
|-- figures
|-- output
|-- R
|   |-- load.R
|   `-- parallel-ipfp.R
`-- spatial-microsim-book.Rproj
```

The example directory structure above is taken from an early version of this
book.  It contains the document for the write-up (`book.Rmd` --- this could
equally be a `.docx` or `.tex` file) and RStudio's `.Rproj` file in the *source
directory*.  The rest of the entities are folders: one for the input data, one
for figures generated, one for data outputs and one for R scripts. The R scripts
should have meaningful names and contain only code that works and is commented.
An additional backup directory could be used to store experimental code. There
is no need to be prescriptive in following this structure, but projects using
spatial microdata tend to be complex, so imposing order over your workflow early
will likely yield dividends in the long run.

The same applies to learning the R language.  Fluency allows complex numerical
ideas to be described with a few keystrokes.  If you are a new R
user it is therefore worth spending some time learning the R language. To this
end the Appendix provides a primer on R from the perspective of spatial
microsimulation.

```{r, echo=FALSE}
# Consider the following expression in the language of mathematics:
# 
# 
# 
# It is easy for experienced R users to translate this into R:
# 
# 
# 
# Note that although the R language is not quite as concise or elegant as
# mathematics, it is certainly faster at conveying the meaning of numerical
# operations than plain English and, in many cases, other programming languages.
# 
# 
# 
# The unusually concise nature of R code is not an accident. It was
# planned to be this way from the outset by its early developers, Robert
# Gentleman and Ross Ihaka, who thought carefully about syntax from the
# outset: "the syntax of a language is important because it determines the
# way that users of the language express themselves" (Ihaka and Gentleman, 2014, p. 300).
# 
# If you are new to R but have some experience with data analysis and
# microsimulation, do not feel intimidated that R is a foreign language.
# As with a spoken language, often the best way to learn is to
# 'jump in the deep end' by living abroad, so learning R through the course
# of this book is certainly an option. However, a deep understanding of R
# will greatly assist understanding the practical elements of the book which
# begin in earnest in [Chapter 4](#DataPrep). Therefore an introductory
# tutorial is provided in [Appendix 1](#apR) which will allow this book
# to focus primarily on the methods of spatial microsimulation and not the
# language in which they are implemented.
```

## Typographic conventions {#typographic}

The following typographic conventions are followed to make the practical
examples easier to follow:

- In-line code is provided in `monospace` font to show it's something the
computer understands.
- Larger blocks of code, referred to as *listings*, are provided on separate lines
and have coloured *syntax highlighting* to distinguish between values, names and functions:

```{r}
x <- c(1, 2, 5, 10) # create a vector
sqrt(x) # find the square root of x
```
 - Output from the *R console* is preceded by the `##` symbol, as illustrated above.
 - Comments are preceded by a single `#` symbol to explain specific lines.
 - Often, reference will be made to files contained within the book's project
 folder. The notation used to refer to the location of these files follows
 the way we refer to files and folders on Linux computers. Thus 'R/CakeMap.R'
 refers to a file titled 'CakeMap.R', within the 'R' directory of the project's
 folder. 
 
There are many ways to write R code that will generate the same results.
However, to ensure clarity and consistency, a single style, advocated in
a [chapter](http://r-pkgs.had.co.nz/style.html) in Hadley
Wickham's *Advanced R*, is followed
throughout
[@wickham2014adv]. 
Consistent style and plentiful comments will make your code
readable by yourself and others for decades to come.

## An overview of the book {#overview}

This document is a working draft of a book to be published in CRC Press's R
Series in summer 2015. Any comments, relating to code, content or
clarity of explanation will be gratefully
received.^[Feedback can be left via email to
r.lovelace@leeds.ac.uk or via the project's GitHub page
(http://github.com/Robinlovelace/spatial-microsim-book).]

The structure is as follows:

- *SimpleWorld*, a 'no nonsense' and reproducible
explanation of spatial microsimulation with reference to an imaginary planet.
- Data preparation, dedicated to the boring but vital task
of loading and 'cleaning' the input data, ready for spatial microsmulation.
- *Preparing input data*, an introduction to reformatting individual
and aggregate-level data ready for spatial microsimulation.
- *Spatial microsimulation in R*, which introduces the main functions
and techniques that are used to generate spatial microdata.
- *CakeMap: spatial microsimulation in the wild* is a larger
and more involved example using real data.
- *Spatial microdata in agent-based models* describes how the output
from spatial microsimulation can be used as an input into more complex models.
- *Additional tools and techniques* introduces further methods
and applications, including a description of R packages for spatial
microsimulation.



# What is spatial microsimulation? {#what-is}

Spatial microsimulation is a
statistical technique for allocating individuals from a survey dataset
to administrative zones, based on shared variables between the areal and
individual level data.
As with many new and infrequently used phrases, this
understanding is not shared by everyone. The meaning of spatial
microsimulation varies depending on context and who you ask. To an
economist, spatial microsimulation is likely to imply
modelling some kind of temporal process such as how individual agents in
different areas respond to changes in prices or policies. To a transport
planner, the term may imply simulating the precise movements of vehicles on
the transport network. To your next door neighbour it may mean you have
started speaking gobbledygook! Hence the need to consider what spatial
microsimulation is, and what it is not, at the outset.

## Terminology {#terminology}

Delving a little into the etymology and history of the term reveals the
reasons behind this duplicity of meaning and highlights the importance
of terminology. Rarely will you be understood saying
“I use *spatial microsimulation*” in everyday life. Usually it
is important to add context. Below are a few hypothetical situations and
suggested responses.

-   When talking to a colleague, a transport modeller: “spatial
    microsimulation, also known as population synthesis...”

-   Speaking to agent based modellers: “we use spatial microsimulation
    to simulate the characteristics of geo-referenced agents...”

-   Communicating with undergraduates who are unlikely to have come
    across the term or its analogies. “I do spatial microsimulation, a
    way of generating individual-level data for small areas...”

-   Chatting casually in the pub or coffee shop: “I’m using a technique
    called spatial microsimulation to model people...”.

The above examples illustrate potential for
confusion: care needs to be taken to use terminology each audience understands. The transport modeller, for example, may
already know that the term 'population synthesis' means creating an individual-level
dataset of real areas, whereas more basic terms need to be used when communicating
the method to policy makers. All this links back to the importance
of transparency and reproducibility of method discussed in the previous chapter:
avoid implying spatial microsimulation is something it is not.

Faced with uncomprehending stares when describing the method, some may
be tempted to ‘blind them with science’, relying on
sophisticated-sounding jargon, for example by saying: “we use simulated
annealing in our integerised spatial microsimulation model”. Such
wording obscures meaning (how many people in the room will understand
‘integerised’, let alone ‘simulated annealing’) and makes the process
inaccessible. Although much jargon is used in the spatial
microsimulation literature and in this book, care must be taken to ensure
that people understand what you are saying.

This raises the question, why use the term spatial microsimulation at
all, if it is understood by so few people? The answer to this is that
spatial microsimulation, defined clearly at the outset and used
correctly, can concisely describe a technique that would otherwise need
many more words on each use. Try replacing ‘spatial microsimulation’
with ‘a statistical technique to allocate individuals from a survey
dataset to administrative zones’ each time it appears in this book and
the advantages of a simple term should become clear! ‘Population
synthesis’ is perhaps a more accurate term but, transport modelling
aside, the literature already uses ‘spatial microsimulation’. Rather
than create more complexity with *another* piece of jargon, we proceed
with the term favoured by the majority of practitioners.

Why has this situation, in which practitioners of a statistical method
must tread carefully to avoid confusing their audience, come about?
First it’s worth stating that the problem is by no means unique to this
field: imagine the difficulties that Bayesian statisticians must
encounter when speaking of prior and posterior probability distributions
to an uninitiated audience. Let alone describing Gibb’s sampling.

To avoid confusion regarding the terminology used in this book,
a glossary defining much of the jargon relating to spatial microsimulation
is provided at the end. For now, to
help answer what spatial microsimulation is we will look at 
its applications and then at what it is not.

## Applications {#applications}

Spatial microsimulation has a wide variety of applications and there
are many areas where the technique has been used.
Some of the main areas of application have
been health, economic policy evaluation and transport. Rather than attempt
to provide a comprehensive account of the range of current and possible
applications, this section describes a single study in each area to exemplify
how spatial microsimulation is used.

### Health applications

An excellent example of the potential practical utility of spatial microsimulation
is a study
which estimated the rate of smoking at the
small area level in the city of Leeds UK
[@Tomintz2008].
Smoking is a classic 'target variable' in spatial microsimulation:
it is reported in a number of individual-level surveys but there is surprisingly
little information about how smoking rates vary from place to place.
Thus it is difficult to where to locate services that depend on the rate of smoking.
The synthetic spatial microdata could thus be used to help identify new clinics
to help people stop smoking. (Alternatively, the spatial microdata could be used
by a tobacco chain to help decide where to invest in a new shop, highlighting
the potential misuse of the technique by unscrupulous analysts.) @Tomintz2008
found that actual anti-smoking clinics were not located optimally. Furthermore,
the results pointed to optimal locations for new clinics, potentially improving
the cost-effectiveness of public health campaigns.

This research has since been 'scaled-up' to estimate smoking rates across the whole
of Austria. The [simSALUD](http://www.simsalud.org/) portal provides users with
access to the resulting spatial microdata and an on-line interface to allow
for the selection of constraint variables and other options to customise the
model for the specific purposes. This portal-based system and the provision
of synthetic spatial microdata to researchers illustrates one possible direction that
spatial microsimulation research could go in, where the synthetic data produced from
a large model is the main output of the research, to be used by others for a variety of
applications.

The example of smoking demonstrates the increase spatial resolution
that spatial microsimulation can bring to bear on under-studied areas in public
health. Where the prevalence of unhealthy activities is closely related 
to socio-demographic variables, a synthetic microdataset can lead to decision
making tools that would be difficult to implement with non-spatial surveys alone.
**Simobesity** is another research project and spatial microsimulation software tool that
estimates the prevalence of obesity at the local levels depending on demographic
constraint variables
[@Edwards2013].
Recent evidence has emerged on the impact of
car-dependent urban environments on inactive lifestyles and resulting poor health
(these areas have been labelled 'obesogenic'). In this context, there is
great potential for combining socio-demographic and environmental-geographic
variables in a spatial microsimulation model. Using the same principles as the
@Tomintz2008 paper, the outputs of such a model could help target local interventions
to tackle physical inactivity, to maximise the benefits of public health funds.

```{r, echo=FALSE}
# TODO: link the reader here to a chapter where smsim models are linked to geo-data
```

### Economic policy evaluation

Microsimulation was originally developed for economic policy evaluation and it
is still one of most common applications. 'Social impact evaluation', where the
impact of policy changes on different income and socio-demographic groups is
explored, is a classic example of applied microsimulation research. Frequently
these simulations are undertaken by government financial or welfare departments
for entire countries and focus on overall shifts in the population rather than
spatial variability in the impacts. EUROMOD...

- EUROMOD
- Income decile graph

### Transport

## What spatial microsimulation is not {#is-not}

**Spatial microsimulation is not strictly spatial**

The most surprising feature of spatial microsimulation
is that the method is not strictly *spatial*. The only
reason why the method has developed this name (as opposed to 'small
area population synthesis', for example) is that practitioners tend
to use administrative zones, which represent geographical areas, as the
grouping variable. However, any mutually exclusive grouping variable,
such as age band or number of bedrooms in your house, could
be used. Likewise, geographical location can be used as a *constraint variable*.
In most spatial microsimulation models, the spatial variable is a mutually
exclusive grouping, interchangeable with any such group. Of course, the spatial
microdata, maps and analysis that result from spatial microsimulation are spatial,
it's just that there is nothing inherently spatial about the method used to
generate the spatial microdata.

To be more precise, spatial microsimulation is not *inherently spatial*.
Spatial attributes such as the geographic coordinates of the zones
to which individuals have been allocated and home and work
locations can easily be added to the spatial microdata after they have been
generated. It is the use of geographical variables as the grouping variable that is
critical here and which distinguishes spatial microsimulation from other types
of microsimulation.

**Spatial microsimulation is not agent-based modelling (ABM).**

Spatial microsimulation does involve the creation and analysis of individuals,
but it does not imply any interaction between these individuals. For this, an
agent-based model (ABM) is needed. It would be easy to assume that because
the method contains the word 'simulation', it includes modelling of individual
behaviours over time and space. This is not the case.

Spatial microsimulation as an approach to modelling involves what-if scenarios,
the allocation of individuals to specific zones and, in some cases, a time
step to represent the individuals' evolution over time. Agent based modelling,
by contrast imply some kind of interaction between the individual units.
Spatial microsimulation is closely linked to ABM, however. As described in
Chapter \ref{ABM}, the outputs of spatial microsimulation form an excellent
basis for ABMs.

**Spatial microsimulation does not generate new data**

During spatial microsimulation, apparently new individuals are created and
placed into zones. It would be tempting to think that new information
about the world is somehow
being created. This is not the case: the 'new' individuals are simply repeats
of individuals we already knew about from the individual-level data, albeit
in a different order and in different combinations. If the population of the
study area is greater than the sample size of the input data, many individuals
will have to be 'cloned'. Thus we are not increasing the diversity of the dataset,
simply changing its aggregate-level characteristics.

## Assumptions {#assumptions}

As with any simulation technique, spatial microsimulation is based on
assumptions, some of which are unlikely to hold in all cases. This
does not preclude spatial microsimulation in cases where the assumptions
do not hold: "It is far better to foresee even without
certainty than not to foresee at all", as Henri Poincaré put it
[@barthelemy2014parallelized].

```{r, echo=FALSE}
# (Barthélemy, 2014).
```


It is vital, however, that users of spatial microsimulation and
'consumers' of the resulting research understand that the results of spatial
microsimulation are not *real* but a best estimate of the population in a
given area. The danger is that if the assumptions are not understood, incorrect
conclusions will result. It is therefore the duty of researchers using spatial
microsimulation (and other techniques) to clearly state the assumptions on which
the results depend on and the extent to which these assumptions can be expected
to hold in practice. Roughly speaking there are four main assumptions underlying
all spatial microsimulation models:

1. The individual-level microdata are representative of the study area.
2. The target variable is dependent on the constraint variables and their
interactions in a way that is relatively constant over space and time.
3. The relationships between the constraint variables are not spatially dependent.
4. The input microdataset and constraints are sufficiently rich and detailed
to reproduce the full diversity of individuals and areas in the study region.

Obviously the real world is complex and many processes are spatially dependent,
invalidating assumptions 2 and 3. The extent to which the relationships between
variables can be deemed to be constant over space is often
unknown. However, there there are ways of
checking the spatial dependency of
relationships between multiply variables, not least Geographically Weighted Regression
(GWR). 

These limitations should be discussed at the outset of spatial
microsimulation research, with reference to the input data. To see how spatial
microsimulation simplifies the real world, the next chapter describes
a hypothetical scenario where 33 inhabitants of an imaginary land
are simulated and allocated to three zones
based on a microdataset of only 5 individuals and two constraint variables.

# SimpleWorld

To see the link between the methodology introduced later in the
book (see Chapter 5) and the various real-world applications, let's take a look at a
simple example of the kind of situation where spatial microsimulation is useful.

We'll use an imaginary world called SimpleWorld, consisting of only 3 zones that
cover the entirety of the SimpleWorld sphere. 

```{r simpleworld,  fig.cap="The SimpleWorld sphere", echo=FALSE, message=FALSE}
# Code to create SimpleWorld
# Builds on this vignette: http://cran.r-project.org/web/packages/sp/vignettes/over.pdf
library(sp)
library(ggplot2)
xpol <- c(-180, -60, -60, -180, -180)
ypol <- c(-70, -70, 70, 70, -70)
pol = SpatialPolygons(list(
  Polygons(list(Polygon(cbind(xpol, ypol))), ID="x1"),
  Polygons(list(Polygon(cbind(xpol + 120, ypol))), ID="x2"),
  Polygons(list(Polygon(cbind(xpol + 240, ypol))), ID="x3")
  ))
# plot(pol)
proj4string(pol) <- CRS("+init=epsg:4326")
pol1 <- fortify(pol)

theme_space_map <- theme_bw() +
  theme(
#     rect = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    panel.grid.major = element_line(size = 3)
  )

ggplot(pol1) + geom_path(aes(long, lat, group, fill = group)) +
    coord_map("ortho", orientation=c(41, -74, 52)) + 
  theme_space_map
```

## SimpleWorld data {#SimpleWorldData}

This is a small world, 
containing 12, 10 and 11 individuals of its alien inhabitants
in zones 1 to 3, respectively: a planetary population of 33.
From the SimpleWorld Census, we know
how many young (under 49 space years old) and old (over 50)
residents live in each
zone, as well their genders: male and female.
This information is displayed in the tables below.

Table: Aggregate-level age counts for SimpleWorld.

|zone   | 0-49 yrs| 50 + yrs|
|:--|-----:|-----:|
|1  |     8|     4|
|2  |     2|     8|
|3  |     7|     4|

Table: Aggregate sex counts for SimpleWorld.

|Zone   |  m|  f|
|:--|--:|--:|
|1  |  6|  6|
|2  |  4|  6|
|3  |  3|  8|

```{r fig = "Mercator maps of the zones in SimpleWorld", echo=FALSE, message=FALSE}
con_age <- read.csv("data/SimpleWorld/age.csv")
con_sex <- read.csv("data/SimpleWorld/sex.csv")
cons <- cbind(con_age, con_sex)

# library(knitr)
# kable(con_age, row.names = T)
# kable(con_sex, row.names = T)

pol <- SpatialPolygonsDataFrame(pol, cons, match.ID = F)

# pol@data
pol$p_young <- pol$a0.49 / (pol$a.50. + pol$a0.49) * 100
pol$p_male <- pol$m / (pol$f + pol$m) * 100

pol$id <- c("x1", "x2", "x3")
library(plyr)
pol1 <- join(pol1, pol@data)
pol1$Name <- paste("Zone", 1:3, sep = " ")
pol1$xpos <- seq(-120, 120, length.out = 3)
pol1$ypos <- 0

# ggplot(pol1) + 
#   geom_polygon(aes(long, lat, group, fill = p_young)) +
#   geom_path(aes(long, lat, group, fill = p_young)) +
#   geom_text(aes(xpos, ypos, label = Name)) +
#   theme_bw() +
#   scale_fill_continuous(low = "black", high = "white", limits = c(0, 100),
#     name = "% Young") +
#   coord_map() 
# 
# ggplot(pol1) + 
#   geom_polygon(aes(long, lat, group, fill = p_male)) +
#   geom_path(aes(long, lat, group, fill = p_male)) +
#   geom_text(aes(xpos, ypos, label = Name)) +
#   theme_bw() +
#   scale_fill_continuous(low = "black", high = "white", limits = c(0, 100),
#     name = "% Male") +
#   coord_map() 
```

Next, imagine a more detailed dataset about 5 of SimpleWorld's
inhabitants, recorded from a survey. This is in a different
form from the aggregate-level data presented in the above tables.
This *microdata* survey contains one row per individual, in contrast
to the *aggregate constraints*, which have one row per zone.
This individual level data includes exact age
(as opposed to the broad categories used in the zone data), as well as income.

\clearpage

```{r, echo=FALSE}
# ind <- read.csv("data/SimpleWorld/ind.csv")
# ind$income <- round(rnorm(n = nrow(ind), mean = 1000, sd = 100))
# ind$income <- ind$income + 30 * ind$age
# ind$income[ ind$age == "f"] <- ind$income + 1000
# write.csv(ind, "data/SimpleWorld/ind-full.csv", row.names = F)
ind <- read.csv("data/SimpleWorld/ind-full.csv")
# kable(ind)
```

Table: Individual-level survey data from SimpleWorld.

| id| age|sex | income|
|--:|---:|:---|------:|
|  1|  59|m   |   2868|
|  2|  54|m   |   2474|
|  3|  35|m   |   2231|
|  4|  73|f   |   3152|
|  5|  49|f   |   2473|

Note that although the microdataset contains additional information about the
inhabitants of SimpleWorld, it lacks geographical information about where each
inhabitant lives or even which zone they are from.  This is typical of
individual-level survey data.  Spatial microsimulation tackles this issue by
allocating individuals from a non-geographical dataset to geographical zones in
another.

## A weight matrix {#weight-matrix}

The procedures we will learn to use in this book do this by allocating *weights*
to each individual for each zone. The higher the weight for a particular
individual-zone combination, the more representative that individual is of that
zone. This information can be represented as a *weight matrix*, such as the one
shown below.

```{r, echo=FALSE, eval=FALSE}
cat_age <- model.matrix(~ ind$age - 1)
cat_sex <- model.matrix(~ ind$sex - 1)[, c(2, 1)]
ind_cat <- cbind(cat_age, cat_sex) # combine flat representations of the data
ind$age <- cut(ind$age, breaks = c(0, 49, 120), labels = c("a0_49", "a50+"))
weights <- matrix(data = NA, nrow = nrow(ind), ncol = nrow(cons))
cons <- apply(cons, 2, as.numeric)
library(ipfp) # load the ipfp library after: install.packages("ipfp")
apply(cons, MARGIN = 1, FUN =  function(x) ipfp(x, t(ind_cat), x0 = rep(1,nrow(ind))))
```

Table: A 'weight matrix' linking the microdata (rows) to the zones (columns)
of SimpleWorld.

| Individual| Zone 1  | Zone 2  | Zone 3  |
|---:|--------:|--------:|--------:|
|   1|    1.228|    1.725|    0.725|
|   2|    1.228|    1.725|    0.725|
|   3|    3.544|    0.550|    1.550|
|   4|    1.544|    4.550|    2.550|
|   5|    4.456|    1.450|    5.450|

The highest value (5.450) is located, to use R's notation, in cell
`weights[5,3]`, the 5th row and 3rd column in the matrix `weights`. This means
that individual number 5 is considered to be highly representative of Zone 3,
given the input data in SimpleWorld.  This makes sense because there are many
(7) young people and many (8) females in Zone 3, relative to the input
microdataset (which contains only 1 young female). The lowest value (0.550) is
found in cell `[3,2]`. Again this makes sense: individual 3 from the
microdataset is a young male yet there are only 2 young people and 4 males in
zone 2. A special feature of the weight matrix above is that each of the column
sums is equal to the total population in each zone.  We will discover how the
weight matrices are generated in Chapter 5.

## Spatial microdata {#spatial-microdata}

A more useful output from spatial microsimulation is what we refer to as
*spatial microdata*.  This is dataset that contains a single row per individual
(as with the input microdata) but also an additional variable indicating where
each individual lives. The challenge is to ensure that the spatial microdataset
is as representative as possible of the aggregate constraints, while only
sampling from a realistic baseline population. A feasible combination of
individuals sampled from the microdata that represent zone 2 is presented in the
table below. The complete spatial microdataset allocates whole individuals to
each zone, resulting in a more or less realistic insight into the inhabitants of
SimpleWorld. This spatial microdataset is also useful
for the purposes of modelling. We will create this
table at the end of Chapter 4.

| id| zone| age|sex | income|
|--:|----:|---:|:---|------:|
|  1|    2|  59|m   |   2868|
|  2|    2|  54|m   |   2474|
|  4|    2|  73|f   |   3152|
|  4|    2|  73|f   |   3152|
|  4|    2|  73|f   |   3152|
|  4|    2|  73|f   |   3152|
|  5|    2|  49|f   |   2473|
|  4|    2|  73|f   |   3152|
|  5|    2|  49|f   |   2473|
|  2|    2|  54|m   |   2474|

Table: Spatial microdata generated for SimpleWorld zone 2.

The table is a reasonable approximation of the inhabitants of zone 2: older
females dominate in both the aggregate (which contains 8 older people and 6
females) and the simulated spatial microdata (which contains 8 older people and
6 females). Note that in addition the constraint variables, we also have
an estimate the income distribution in SimpleWorld's second zone.

Towards the end of Chapter 4 we will learn how to generate
this table from first principles. The remainder of this section
considers how the outputs of spatial microsimulation, in the context of
SimpleWorld, can be useful before progressing to the practicalities.

```{r, echo=FALSE}
# Add section link here!
```

## SimpleWorld in context {#SimpleWorldContext}

Even though the datasets are tiny in SimpleWorld, we have already generated some
useful output. We can estimate, for example, the average income in each zone.
Furthermore, we could create an estimate of the *distribution* of income in each
area. Although these estimates are unlikely to be very accurate due to the
paucity of data, the methods could be very useful if performed on larger
datasets from the RealWorld (planet Earth).  Finally, the spatial microdata
presented in the above table can be used as an input into an agent-based model
(ABM). Assuming the inhabitants of SimpleWorld are more predictable than those
of RealWorld, the outputs from such a model could be very useful indeed, for
example for predicting future outcomes of current patterns of behaviour.

In addition to clarifying the advantages of spatial microsimulation, the above
example also flags some limitations of the methodology: spatial microsimulation
will only yield useful results if the input microdataset is representative of
the population as a whole, and for each region. If the relationship between age
sex is markedly different in one zone compared with what we assume to be the
global averages of the input data, for example, our estimates could be way out.
Using such a small sample, one could rightly argue, how could the diversity of
33 inhabitants of SimpleWorld be represented by our simulated spatial microdata?
This question is equally applicable to larger simulations. These issues are
important and will be tackled in a subsequent Section (\ref{svalidation}) on
validation.

# Preparing input data {#DataPrep}

```{r, echo=FALSE}
# Applications of spatial microsimulation - to be completed!

## Updating cross-tabulated census data

## Economic forecasting

## Small area estimation

## Transport modelling

## Dynamic spatial microsimulation

## An input into agent based models
```


```{r, echo=FALSE}
# With the foundations built in the previous chapters now (hopefully) firmly in-place,
# we progress in this chapter to actually *run* a spatial microsimulation model
# This is designed to form the foundation of a spatial microsimulation course.
```


```{r, echo=FALSE}
# This next chapter is where people get their hands dirty for the first time -
# could be the beginning of part 2 if the book's divided into parts. 
```

This chapter focuses on the input datasets needed for spatial microsimulation.
Correctly loading, manipulating and assessing these datasets will be critical to
the performance of your models and the ease of modifying them to include new
inputs.  Fortunately R is an accomplished tool for data reformatting,
as we shall see. The data objects loaded in the following steps also
provide the basis for Chapter 5, in which we
undertake spatial microsimulation.

```{r, echo=FALSE}
## Accessing the input data
```

As with most spatial microsimulation models, the input consists of
microdata --- a non-geographical individual-level dataset --- and the
constraint table which represents a series of geographical zones.
The data used in this Chapter and throughout the book can be
downloaded from the book's [GitHub repository](https://github.com/Robinlovelace/spatial-microsim-book). From this page, click on the
'Download ZIP' button to the right and extract the folder into a
sensible place on your computer, such as the Desktop.  From there, you will want
to run R from the project's root directory: open the folder in a file browser
and double click on `spatial-microsim-book.Rproj`.  This should cause RStudio to
be opened at this location, with all the input data files easily accessible to R
through *relative file paths*.

```{r studio,  fig.cap="The RStudio interface with the 'spatial-microsim-book' project loaded", echo=FALSE, message=FALSE}
grid.raster(readPNG("figures/studio-basic.png"))
```


To ease reproducibility of the analysis when working with real data, it is
recommended that the process begins with a copy of the *raw* dataset on one's
hard disc.  Rather than modifying this file, modified ('cleaned') versions
should be saved as separate files. This ensures that after any mistakes, one can
always recover information that otherwise could have been lost and makes the
project fully reproducible. In this chapter, however, a relatively clean and
very tiny dataset from SimpleWorld is used.  We will see in Chapter
\ref{CakeMap} how to deal with larger and more messy data.  Here the focus is on
the principles.

```{r, echo=FALSE}
# It sounds trivial, but the *precise* origin of the input data
# should be described. Comments in code that loads the data (and resulting publications),
# allows you or others to recall the raw information. # going on a little -> rm
# Show directory structure plot from Gillespie here
```

The process of loading, checking and preparing the input datasets for spatial
microsimulation is generally a linear process, encapsulating the following
stages:

1. Load original data 
2. Remove excess information
3. Re-categorise individual-level data
4. Set variable and value names
5. 'Flatten' individual-level data 

'Stripping down' the datasets so that they only contain the bare essential
information will enable you to focus solely on the data that you are interested
in. This is not covered in this chapter because the input datasets are already
extremely bare and because the process should be obvious.

We start with the individual-level dataset for a reason: this dataset is often
more problematic to format than the constraint variables, so it is worth
becoming acquainted with it at the outset. Of course, it is possible that the
data you have are not suitable for spatial microsimulation because they lack
sufficient constraint variables with shared categories in both individual and
aggregate level tables. We assume that you have already checked this. The
checking process for the datasets used in this chapter is simple: both aggregate
and individual-level tables contain age and sex, so they can by combined. Let us
proceed to load some data saved on our hard disc into R's *environment*, where
it is available in memory.

## Loading input data {#Loading} 

Real-world individual-level data may be provided in a variety of formats
but ultimately needs to be loaded into R as a *data frame* object.

In this case the dataset is loaded from a `.csv` file:

```{r}
# Load the individual-level data
ind <- read.csv("data/SimpleWorld/ind.csv") 
class(ind) # verify the data type of the object
ind # print the individual-level data
```

```{r, echo=FALSE}
### Loading and checking aggregate-level data
```

Constraint data are usually made available one variable at a time,
so these are read in one file at a time:

```{r}
con_age <- read.csv("data/SimpleWorld/age.csv")
con_sex <- read.csv("data/SimpleWorld/sex.csv")
```

We have loaded the aggregate constraints. As with the individual level data, is
worth inspecting each object to ensure that they make sense before continuing.
Taking a look at `age_con`, we can see that this data set consists of 2
variables for 3 zones:

```{r}
con_age
```

This tells us that there 12, 10 and 11 individuals in zones 1, 2 and 3,
respectively, with different proportions of young and old people. Zone 2, for
example, is heavily dominated by older people: there are 8 people over 50 whilst
there are only 2 young people (under 49) in the zone.

Even at this stage there is a potential for errors to be introduced.  A classic
mistake with areal data is that the order in which zones are loaded changes from
one table to the next. The constraint data should therefore come with some kind of *zone
id*, an identifying code that will eventually allow the attribute data to be
combined with polygon shapes in GIS software.

```{r, echo=FALSE}
# Make the constraint data contain an 'id' column, possibly scrambled 
```

If we're sure that the row numbers match between the age and sex tables (we are
sure in this case), the next important test is to check that the total
populations are equal for both sets of variables.  Ideally both the *total*
study area populations and *row totals* should match. If the *row totals* match,
this is a very good sign that not only confirms that the zones are listed in the
same order, but also that each variable is sampling from the same *population
base*. These tests are conducted in the following lines of code:

```{r}
sum(con_age)
sum(con_sex) 

rowSums(con_age)
rowSums(con_sex)
rowSums(con_age) == rowSums(con_sex)
```

The results of the previous operations are encouraging. The total population is
the same for each constraint overall and for each area (row) for both
constraints.  If the total populations between constraint variables do not match
(e.g. because the population bases are different) this is problematic.
Appropriate steps to normalise the errant constraint variables are described in
the CakeMap Chapter (\ref{CakeMap}).

## Subsetting to remove excess information {#subsetting-prep}

In the above code, `data.frame` objects containing precisely the information
required for the next stage were loaded.  More often, superfluous information
will need to be removed from the data and subsets taken. It is worth removing
superfluous variables earl, to avoid over-complicating and slowing-down the
analysis.  For example, if `ind` had 100 variables of which only the 1st, 3rd and 4th were of
interest (in that they match the constraint variables), the following command could be used to update the object. Only the relevant variables corresponding to
columns (1, 3 and 4 in this case) are retained: `ind <- ind[, c(1, 3, 4)]`.
Alternatively, `ind$age <- NULL` removes the age variable.

Although `ind` is small and simple it will behave in the same way as a much
larger dataset, providing opportunities for testing subsetting syntax in R.  It
is common, for example, to take a subset of the working *population base*: those
aged 16 and 74 in full-time employment. Methods for doing this are provided in
the Appendix ([](#subsetting)).

## Re-cateorising individual-level variables {#re-categorise}

Before transforming the individual-level dataset `ind` into a form that can be
compared with the aggregate-level constraints, we must ensure that each dataset
contains the same information. It can be more challenging to re-categorise
individual-level variables than to re-name or combine aggregate-level variables,
so the former should usually be set first.  An obvious difference between the
individual and aggregate versions of the `age` variable is that the former is of
type `integer` whereas the latter is composed of discrete bins: 0 to 49 and 50+.
We can categories the variable into these bins using 
`cut()`:^[The
combination of curved and square brackets in the output from the `cut` function
may seem strange but this is
an International Standard: curved brackets mean 'excluding' and square brackets
mean 'including'. The output `(49, 120]`, for example, means 'from
49 (excluding 49, but including 49.001) to 120 (including the exact value 120)'.
See http://en.wikipedia.org/wiki/ISO_31-11 for more
on international standards on mathematical notation.]

```{r}
# Test binning the age variable
cut(ind$age, breaks = c(0, 49, 120))
```

If we wanted to change these category labels to something more readable,
we can do this by adding another argument to the `cut` function:

```{r}
# Convert age into a categorical variable with user-chosen labels
(ind$age <- cut(ind$age, breaks = c(0, 49, 120),
  labels = c("a0_49", "a50+")))
```

Users should be ware that `cut` results in a vector of class *factor*, which
can cause problems later down the line.

## Matching individual and aggregate level data names {#matching}

Before combining the newly recategorised individual-level data with the
aggregate constraints, it is useful to for the category labels to match up.
This may seem trivial, but will save time in the long run. Here is the problem:

```{r}
levels(ind$age)
names(con_age)
```

Note that the names are subtly different. To solve this issue, we can
simply change the names of the constraint variable, assuming they
are in the correct order:

```{r}
names(con_age) <- levels(ind$age) # rename aggregate variables
```

With both the age and sex constraint variable names now matching the
category labels of the individual-level data, we can proceed to create a
single constraint object we label `cons`. We do this with `cbind()`:

```{r}
cons <- cbind(con_age, con_sex)
cons[1:2, ] # display the constraints for the first two zones
```

## 'Flattening' the individual level data {#flattening}

We have made steps towards combining the individual and aggregate datasets and
now only need to deal with 2 objects (`ind` and `cons`) which now share
category and variable names.
However, these datasets cannot possibly be compared because they are of different dimensions:

```{r}
dim(ind)
dim(cons)
```

The above code confirms this: we have one individual-level dataset comprising 5
individuals with 3 variables (2 of which are constraint variables and the other an ID) and one
aggregate-level constraint table called `cons`, representing 3 zones
with count data for 4 categories across 2 variables.

The dimensions of at least one of these objects must change
before they can be easily compared. To do this
we 'flatten' the individual-level dataset;
this means increasing its width
to match the constraint data.
This is a two-stage process. First,
`model.matrix()` is used to expand each variable into the number of columns as there are categories in each and assign. 
Knoblauch and Maloney (2012) provide a lengthier description of this
which is available online, for free.

Second, `colSums()` is used to take the sum of each column.^[As we shall see in Section \ref{ipfp},
only the former of these is needed if we use the
**ipfp** package for re-weighting the data, but both are presented to enable
a better understanding of how IPF works.]

```{r}
cat_age <- model.matrix(~ ind$age - 1)
cat_sex <- model.matrix(~ ind$sex - 1)[, c(2, 1)]

 # Combine age and sex category columns into single data frame
(ind_cat <- cbind(cat_age, cat_sex)) # brackets -> print result
```

Note that second call to `model.matrix` is suffixed with `[, c(2, 1)]`.
This is to swap the order of the columns: the column variables are produced
from `model.matrix` is alphabetic, whereas the order in which the variables
have been saved in the constraints object `cons` is `male` then `female`.
Such subtleties can be hard to notice yet completely change one's results
so be warned: the output from `model.matrix` will not always be compatible
with the constraint variables.

To check that the code worked properly,
let's count the number of individuals
represented in the new `ind_cat` variable, using `colSums`:

```{r}
colSums(ind_cat) # view the aggregated version of ind
ind_agg <- colSums(ind_cat) # save the result
```

The sum of both age and sex variables is 5 
(the total number of individuals): it worked! 
Looking at `ind_agg`, it is also clear that the object has the same 'width',
or number of columns,
`cons`. This means that the individual-level data can now be compared with
the aggregate-level data. We can check this by inspecting
each object (e.g. via `View(ind_agg)`). A more rigorous test is to see
if `ind_agg` can be combined with `ind_agg`, using `rbind`:

```{r}
rbind(cons[1,], ind_agg) # test compatibility of ind_agg and cons
```

If no error message is displayed on you computer, the answer is yes.
This shows us a direct comparison between the number of people in each
category of the constraint variables in zone and and in the individual level
dataset overall. Clearly, the fit is not very good, with only 5 individuals
in total existing in `ind_agg` (the total for each constraint) and 12
in zone 1. We can measure the size of this difference using measures of
*goodnes of fit*. A simple measure is total absolute error (TAE), calculated in this
case as `sum(abs(cons[1,] - ind_agg))`: the sum of the positive differences
between cell values in the individual and aggregate level data.

The purpose of the *reweighting* procedure in spatial microsimulation is
to minimise this difference (as measured in TAE above)
by adding high weights to the most representative individuals.


# Spatial microsimulation in R

In this chapter we progress from loading and preparing the input data to
running a spatial microsimulation model.
The SimpleWorld data, loaded in the previous chapter,
is used. Being small and simple, the example enables understanding
the process on a 'human scale' and allows experimentation
without the worry of overloading your computer.
However, the methods apply equally to larger and more complex projects.
Therefore practicing the basic principles and methods of spatial microsimulation
in R is the focus of this chapter.
Time spent mastering these basics will make subsequent steps
much easier.

```{r, echo=FALSE}
# How representative each individual is of each zone is determined by their
# *weight* for that zone. If we have `nrow(cons)` zones and `nrow(ind)`
# individuals (3 and 5, respectively, in SimpleWorld) we will create
# 15 weights. Real world datasets (e.g. that presented in chapter xxx)
# could contain 10,000 individuals
# to be allocated to 500 zones, resulting in an unwieldy 5 million element
# weight matrix but we'll stick with the SimpleWorld dataset here for simplicity.
load("cache-data-prep.RData")
```

How representative each individual is of each zone is determined by their
*weight* for that zone. If we have `nrow(cons)` zones and `nrow(ind)`
individuals (3 and 5, respectively, in SimpleWorld) we will create
15 weights. To start, we create an empty weight matrix, ready to be filled
with numbers calculated through the IPF procedure:

```{r}
weights <- matrix(data = NA, nrow = nrow(ind), ncol = nrow(cons))
dim(weights) # dimension of weight matrix: 5 rows by 3 columns
```

## IPF in R {#IpfinR}

The most established *deterministic* method to allocate individuals to zones
is iterative proportional fitting (IPF). IPF involves calculating a series of
weights that represent how representative each individual is of each zone.
This is *reweighting*. The IPF algorithm can be written in
R from scratch, as illustrated in Lovelace (2014), and as implemented in the
smsim-course GitHub repository (https://github.com/Robinlovelace/spatial-microsim-book).
The code in this file, and the accompanying text, saves the weight
matrix after every constraint for each iteration. While this makes the
code relatively slow, it is useful for diagnosing issues with the reweighting
process. However, to save computer and researcher time, we will
skip directly to an alternative method that uses the **ipfp** package
in this section.

Regardless of which implementation of used,
IPF can be used to allocate the individual-level data loaded in
the previous chapter to the three zones of SimpleWorld. IPF is mature,
fast and has a long history.
Interested readers are directed towards recent papers (e.g. Lovelace and Ballas,
2012;
Pritchard and Miller,
2012)
for more detail on the method and its underlying theory.

```{r, echo=FALSE}
# Possibly more on IPF here. For now, press on
```

## Reweighting with **ipfp** {#ipfp}

IPF runs much faster and with less code using the
**ipfp** package than in pure R. The `ipfp` function runs the IPF algorithm
in the C language, taking aggregate constraints, individual level
data and and an initial weight vector (`x0`) as inputs:

```{r}
library(ipfp) # load ipfp library after install.packages("ipfp")
cons <- apply(cons, 2, as.numeric) # to 1d numeric data type
ipfp(cons[1,], t(ind_cat), x0 = rep(1, nrow(ind))) # run IPF
```

It is impressive that the entire IPF process, which takes dozens of lines of
code in pure R can been condensed into two lines: one to
convert the input constraint dataset to `numeric`^[The integer data type fails
because C requires `numeric` data to be converted into its *floating point*
data class.]
and one to perform the IPF operation itself. Note also that although
we did not specify how many iterations to run, the above command
ran the default of `maxit = 1000` iterations, despite convergence happening after
10 iterations. This can be seen by specifying the `maxit` and `verbose` arguments
(the latter of which can be referred to lazily as `v`) in `ipfp`, as illustrated below (only the first line of R output is shown):

```{r, eval=FALSE}
ipfp(cons[1,], t(ind_cat), rep(1, nrow(ind)), maxit = 20, v = T)
```

```
## iteration 0:   0.141421
## iteration 1:    0.00367328
## iteration 2:  9.54727e-05
## ...
## iteration 9:  4.96507e-16
## iteration 10:	4.96507e-16
```

Notice also that a *transposed* (via the `t()` function) version of the individual-level
data (`ind_cat`) is used in `ipfp`
to represent the individual-level data, instead of the
`ind_agg` object used in the pure R version. To prevent having to transpose
`ind_cat` every time `ipfp` is called, save the transposed version:

```{r}
ind_catt <- t(ind_cat) # save transposed version of ind_cat
```

Another object that can be saved prior to running `ipfp` on all zones
(the rows of `cons`) is `rep(1, nrow(ind))`, simply a series of ones - one for each individual.
We will call this object `x0` as its argument name representing
the starting point of the weight estimates in `ipfp`:

```{r}
x0 <- rep(1, nrow(ind)) # save the initial vector
```

To extend this process to all three zones we can wrap the line beginning
`ipfp(...)` inside a `for` loop, saving the results each time into the
weight variable we created earlier:

```{r}
weights_maxit_2 <- weights # create a copy of the weights object
for(i in 1:ncol(weights)){
  weights_maxit_2[,i] <- ipfp(cons[i,], ind_catt, x0, maxit = 2)
}
```

The above code uses `i` to iterate through the constraints, one row (zone) at
a time, saving the output vector of weights for the individuals into columns
of the weight matrix. To make this process even more concise (albeit
less clear to R beginners), we can use R's internal
`for` loop, `apply`:

```{r}
weights <- apply(cons, MARGIN = 1, FUN = 
    function(x) ipfp(x, ind_catt, x0, maxit = 20))
```

In the above code R iterates through each row
(hence the second argument `MARGIN` being `1`, `MARGIN = 2`
would signify column-wise iteration).
Thus `ipfp` is applied to each zone in turn, as with the `for` loop implementation. 
The speed savings of writing the function 
with different configurations are benchmarked in
'parallel-ipfp.R' in the 'R' folder of the book project directory.
This shows that reducing the maximum iterations of `ipfp` from
the default 1000 to 20 has the greatest performance benefit.^[These
tests also show that any speed gains from using `apply` instead of `for` are negligible, so
whether to use `for` or `apply` can be decided by personal preference.]
To make the code run faster on large datasets, a parallel version of
`apply` called `parApply` can be used. This is also
tested in 'parallel-ipfp.R'.

```{r, echo=FALSE, eval=FALSE}
# Also discuss what happens when you get a huge dataset, from Stephen's dataset
```

It is important to check that the weights obtained from IPF make sense.
To do this, we multiply the weights of each individual by rows of
the `ind_cat` matrix, for each zone. Again, this can be done using
a for loop, but the apply method is more concise:

```{r}
ind_agg <- t(apply(weights, 2, function(x) colSums(x * ind_cat)))
colnames(ind_agg) <- colnames(cons) # make the column names equal
```

As a preliminary test of fit,
it makes sense to check a sample of the aggregated weighted data
(`ind_agg`) against the same sample of the constraints.
Let's look at the results (one would use a subset of the results, 
e.g. `ind_agg[1:3, 1:5] for the first five values of the first 3
zones for larger constraint tables found in the real world):

```{r}
ind_agg
cons
```

This is a good result: the constraints perfectly match the results
generated using ipf, at least for the sample. To check that this
is due to the `ipfp` algorithm improving the weights with each iteration,
let us analyse the aggregate results generated from the alternative
set of weights, generated with only 3 iterations of IPF:

```{r}
# Re-allocate the values of ind_agg (not column names - note '[]')
ind_agg[] <- t(apply(weights_maxit_2, MARGIN = 2, 
  FUN = function(x) colSums(x * ind_cat)))
ind_agg[1:2, 1:3]
```

Clearly the final weights after 3 iterations of IPF represent the constraint
variables well, but do not match perfectly except in the second constraint. This shows the importance of
considering number of iterations in the reweighting stage --- too many iterations
can be wasteful, too few may result in poor results. To reiterate,
20 iterations of IPF should be sufficient in most cases for the results
to converge towards their final level of fit. 
More sophisticated ways of evaluating model fit are
presented in Section \ref{svalidation}.

## Alternative reweighting algorithms {#alternative-reweighting}

As described in the Introduction, IPF is just one strategy for obtaining
a spatial microdataset. However, researchers (myself included) have tended to
select one method that they are comfortable and stick with that for their models.
This is understandable because setting-up the method is usually time consuming:
most researchers rightly focus on applying the methods to the real world rather
than fretting about the details. On the other hand, if alternative methods
work better for a particular application, resistance to change can result
in poor model fit. In the case of very large datasets, spatial microsimulation
may not be possible unless certain methods, optimised to deal with large
datasets, are used. Above all, there is no consensus about which methods
are 'best' for different applications, so it is worth experimenting to identify
which method is most suitable for each application.

### Spatial microsimulation as an objective function

In general terms, an *objective function* is a formula that processes a series of
input variables ($x$) using a number of parameters ($par$) to generate
a single numeric output:

$$ \mbox{minimise } f_0(x) $$

$$ \mbox{subject to } f_i(x) \leq par_i,\  i = 1, ..., m $$

The goal of objective function $f_0$ such as that illustrated above is to
enable the selection of the parameters $par_i$ to $par_m$
which minimize the result. The values that parameters may take are constrained
by *constraint functions* ($f_i$) or fixed values (Boyd and
Vandenberghe, 2009). The case of spatial microsimulation has relatively
simple constraints: they are all positive or zero.
Seeing spatial microsimulation as an objective function
allows solutions to be found using established
techniques of *constrained optimisation*.
The main advantage of this re-framing
is that it allows any optimisation algorithm to perform the reweighting.
Key to this is interpreting individual weights as parameters (the vector $par$,
of length $m$ above)
that are iteratively modified to optimise the fit between individual and
aggregate-level data. The measure of fit is we use in this context is
Total Absolute Error (TAE) and the objective function
is as follows:

$$\mbox{minimise }f(par) = TAE(sim, con)$$

$$\mbox{where }sim = colSums(ind\_cat * par) $$

$$\mbox{subject to } par \geq 0$$

```{r, echo=FALSE}
# how to write in maths?
# $$\mbox{minimise }f(par)$$
```


Note that in the above, $par$ is equivalent to the `weights` object
we have created in previous sections to represent how representative
each individual is of each zone. 
The main issue with this definition of reweighting is therefore the large
number of free parameters: equal to the number of individual-level dataset.
Clearly this can be very very large. To overcome this issue,
we must 'compress' the individual level dataset to its essence, to contain
only unique individuals with respect to the constraint variables
(*constraint-unique* individuals).

The challenge is to convert the binary 'model matrix' form of the
individual-level data (`ind_cat` in the previous examples) into
a new matrix (`ind_num`) that has fewer rows of data. Information about the
frequency of each constraint-unique individual is kept by increasing the
value of the '1' entries for each column for the replicated individuals
by the number of other individuals who share the same combination of attributes.
This may sound quite simple, so let's use the example of SimpleWorld to
illustrate the point.

### Reweighting with optim and GenSA

The base R function `optim` provides a general purpose optimisation framework
for numerically solving objective functions. Based on the objective function
for spatial microsimulation described above,
we can use any general optimisation algorithm for reweighting the
individual-level dataset. But which to use?

Different reweighting strategies are suitable in different contexts and there
is no clear winner for every occasion. However, testing a range of
strategy makes it clear that certain algorithms are more efficient than
others for spatial microsimulation. Figure x demonstrates this variability
by plotting total absolute error as a function of number of iterations for
various optimisation algorithms available from the base function
`optim` and the **GenSA** package.

![](figures/optim-its.png)

```{r, eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
source("R/optim-tests-SimpleWorld.R", echo = FALSE, print.eval = FALSE )
qplot(data = opt_res, time, fit, color = algorithm, geom="line") +
  ylab("Total Absolute Error") + xlab("Time (microseconds)") + scale_color_brewer(palette = 2, type = "qual") + theme_classic() + xlim(c(0, 15000))
```

Figure x shows the advantage of the IPF algorithm we have been using, which
converges rapidly to zero error after only a few iterations.
On the other end of the spectrum is R's default optimisation algorithm,
the Nelder-Mead method. Although the graph shows no improvement
from one iteration to the next, it should be stated that the algorithm
is just 'warming up' at this stage and than each iteration is very
fast, as we shall see. After a few *hundred* iterations (which happen
in the same time that other algorithms take for a single iteration),
the Nelder-Mead method is effective,
converging to close to zero in 400 iterations, taking far more
iterations to converge to a value approximating zero than IPF.
Next best in terms of iterations is `GenSA`, the Generalized Simulated
Annealing Function from the **GenSA** package. GenSA
attained a near-perfect fit after only two full
iterations.

The remaining algorithms shown are, like Nelder-Mead, available from within R's
default optimisation function `optim`. The implementations with `method =` set
to `"BFGS"` (short for the Broyden–Fletcher–Goldfarb–Shanno algorithm),
`"CG"` ('conjugate gradients') performed roughly the same, steadily approaching
but not reaching zero error after five iterations. Finally, the `SANN` method,
also available in `optim`, performed most erratically of the methods tested.
This is another implementation of simulated annealing which demonstrates that
optimisiation functions that depend on random numbers do not always lead to
improved fit from one iteration to the next.

The code used to test these alternative methods for reweighting are provided
in the script 'R/optim-tests-SimpleWorld.R'. The results
should be reproducible on
any computer, provided the book's supplementary materials have been downloaded.
There are many other optimisation algorithms available in R through a wide
range of packages; new and improved functions are available all the time
so enthusiastic readers are encouraged to experiment with this script files:
it is perfectly feasible that an algorithm exists which outperforms all of
those tested for this book. Also, it should be noted that the algorithms
were tested on the extremely simple and rather contrived example dataset
of SimpleWorld. Some algorithms perform better with larger datasets than others
and the results can be highly sensitive to changes to the initial conditions
such as the problem of 'empty cells'.

```{r, echo=FALSE}
# TODO: cite performance testing paper here
```

Therefore these results, as with any modelling exercise,
should be interpreted with a healthy dose of skepticism: just because an
algorithm converges after few 'iterations' this does not mean it is
inherently any faster or more useful than another. The results are context
specific, so it is recommended that the tested framework
in 'R/optim-tests-SimpleWorld.R' is used as a basis for further tests
on algorithm performance on the datasets you are using.
IPF has performed well in the situations I have tested it in (especially
via the `ipfp` function, which performs disproportionately faster
than the pure R implementation on large datasets) but this does not mean
that it is always the best approach.

To overcome the caveat that the meaning of an 'iteration' changes dramatically
from one algorithm to the next, further tests measured the time taken
for each reweighting algorithm to run. The results are very different when
iterations are seen as the independent variable (Figure xx). This figure
Demonstrates that Nelder-Mead is fast at reaching a good
approximation of the constraint data, despite taking many iterations.
`GenSA`, on the other hand, is shown to be much slower than the others,
despite only requiring 2 iterations to arrive at a good level of fit.
The `SANN` method is not shown at all due to high TAE values for all
times shown.

![](figures/optim-time.png)


### Combinatorial optimisation

Combinatorial optimisation is an alternative to IPF for allocating individuals
to zones. This strategy is *probabilistic*
and results in integer weights, as opposed to the fractional weights
generated by IPF. Combinatorial optimisation may be more appropriate
for applications where input individual microdatasets are very large:
the speed benefits of using the deterministic IPF algorithm shrink as the size of
the survey dataset increases. An alternative to combinatorial optimisation,
which builds on the fractional weights generated through IPF
(or any other method of reweighting), is *intersation*.
Integerisation largely reduces the need for combinatorial optimisation,
as we shall see in the next section (\ref{sintegerisation}).

There are two approaches for reweighting using combinatorial optimisation
in R: shuffling individuals in and out of each area and combinatorial optimisation
the *domain* of the solution space set to allow inter-only results...

The second approach to combinatorial optimisation in R depends on methods
that allow only integer solutions to the general constrained optimisation
formulae demonstrated in the previous section. *Integer programming* is the
branch of computer science dedicated to this area, and it is associated with
its own algorithms and approaches, some of which have been implemented in R
(Zubizarreta, 2012).

To illustrate how the approach works in general terms, we can use the
`data.type.int` argument of the `genoud` function in the **rgenoud** package.
This ensures only integer results for
a genetic algorithm to select parameters are selected:

```{r, eval=FALSE}
set.seed(2014)
genoud(nvars = nrow(indu), fn = fun, ind_num = indu, con = cons[1,],
  control = list(maxit = 1000), data.type.int = TRUE,
  
  # Set min and maximum values of constraints with 'Domains'
  D = matrix(c(rep(0, nrow(indu)),rep(100, nrow(indu))), ncol = 2))
```

This rather long command, implemented in 'optim-tests-SimpleWorld.R',
results in weights for the unique individuals 1 to 4 of 1, 4, 2 and 4 respectively.
This means that a total population of 11 individuals is simulated for the zone,
composed of 1 instance of individual 1 and 'clones' of the others.
Given that the command takes 5 seconds to generate
weights for only 4 unique individuals, and the fact that there are in fact
12 not 11 individuals in zone 1 in SimpleWorld, this implementation of `genoud`
is clearly not production ready.
`genoud` is used here to provide a practical demonstration of the possibilities of
combinatorial optimisation in R, an area that has great potential to advance
in the future.

For combinatorial optimisation algorithms designed for spatial microsimulation
we must, for now, look for programs outside the R 'ecosystem'.
Harland (2013) provides a practical tutorial
introducing the subject based on the Flexible Modelling Framework (FMF)
Java program. It is possible to generate integer spatial microdata using IPF, however,
using a technique we refer to imaginatively as integerisation.
This is the topic of the next section.

## Integerisation {#sintegerisation}

Integerisation is the process by which a vector of real numbers
is converted into a vector of integers corresponding to the
individuals present in synthetic spatial microdata.
The length of the new vector must equal the population of the zone
in question and individuals with high weights must be sampled
proportionally more frequently than those with
low weights for the operation to be effective.
The following example illustrates how the process,
when seen as a function called $int$ would work
on a vector of 3 weights:

$$w_1 = (0.333, 0.667, 3)$$

$$int(w_1) = (2, 3, 3, 3)$$

This result was obtained by calculating the sum of the weights (4, which
represents the total population of the zone) and sampling from these
until the total population is reached. In this case individual 2 is selected
once as they have a weight approaching 1, individual 3 was replicated
(*cloned*) three times and individual 1 does not appear in the integerised
dataset at all, as it has a low weight. In this case the outcome is straightforward
because the numbers are small and simple. But what about in less clear-cut cases,
such as $w_2 = (1.333, 1.333, 1.333)$? What is needed is an algorithm to undertake this
process of *integerisation* in a systematic way to maximise the fit between
the synthetic and constraint data for each zone.

In fact there are a number of integerisation strategies available.
Lovelace and Ballas (2012) tested 5 of these and found that probabilistic
integerisation methods consistently outperformed deterministic rivals.
The details of these algorithms are described in the aforementioned paper
and code is provided in the Supplementary Information. For the purposes of
this course we will create a function to undertake the simplest of these,
*proportional probabilities*:

```{r}
int_pp <- function(x){
  sample(length(x), size = round(sum(x)), prob = x, replace = T)
}
```

To test this function let's try it on the vectors of length 3 described in
code:

```{r}
set.seed(0)
int_pp(x = c(0.333, 0.667, 3))
int_pp(x = c(1.333, 1.333, 1.333))
```

The first result was the same as that obtained through intuition; the second
result represented individual 1 being clones three times, plus one instance of individual 2. This is
not intuitive: one would expect at least one of each individual given that they all
have the same weight.

It is important to emphasise that the results will change each time the code is
run, because `sample` is a probabilistic (its output depends on a random number
generator): changing the value inside the brackets proceeding `set.seed`
results in many other combinations of individuals being selected --- test this
out in your code.  This happens because the method relies on *pseudo random
numbers* to select values probabilistically and `set.seed` specifies where the
random number sequence should begin, ensuring repeat-ability. An issue with the
*proportional probabilities* (PP) strategy is that completely unrepresentative
combinations of individuals have a non-zero probability of being sampled: the
method will output $(1, 1, 1, 1)$ once in every 21 thousand runs for $w_1$ and
once every $81$ runs for $w_2$, the same probability as for all other 81 ($3^4$)
permutations.

To overcome this issue Lovelace and Ballas (2012) developed a method which
ensures that any individual with a weight above 1 would be sampled at least once,
making the result $(1, 1, 1, 1)$ impossible in both cases.  This method is
*truncate, replicate, sample* (TRS) integerisation:

```{r}
int_trs <- function(x){
  truncated <- which(x >= 1)
  replicated <- rep(truncated, floor(x[truncated]))
  r <- x - floor(x)
  def <- round(sum(x)) - length(replicated) # deficit population
  if(def == 0){
    out <- replicated
  } else {
    out <- c(replicated,
      sample(length(x), size = def, prob = r, replace = FALSE))
  }
  out
}
```

To see how this new integerisation method and associated R function
performed, we will run it on the same input vectors:

```{r}
int_trs(c(0.333, 0.667, 3))
int_trs(c(1.333, 1.333, 1.333))
```

The range of possible outcomes is smaller using the TRS technique; the fit
between the resulting microdata and the aggregate constraints will tend to be
higher. Thus we use the TRS methodology, implemented through the function
`int_trs`, for integerising the weights generated by IPF throughout the majority
of this book.

Let's use TRS to
generate spatial microdata for SimpleWorld.  Remember, we already have generated
the weight matrix `weights`.  The only challenge is to save the vector of
sampled individual id numbers, alongside the zone number, into a single object
from which the attributes of these individuals can be recovered. Two strategies
for doing this are presented in the code below:

```{r}
# Method 1: using a for loop
ints_df <- NULL
for(i in 1:nrow(cons)){
  ints <- int_trs(weights[, i])
  ints_df <- rbind(ints_df, data.frame(id = ints, zone = i))
}

# Method 2: using apply
ints <- unlist(apply(weights, 2, int_trs)) # integerised result
ints_df <- data.frame(id = ints,
  zone = rep(1:nrow(cons), colSums(weights)))
```

Both methods yield the same result for `ints_df`. The only differences being
that Method 1 is perhaps more explicit and easier to understand whilst Method 2
is more concise.

The final remaining step is to re-allocate the attribute data from the
individual-level data back into `ints_df`. To do this we use the `inner_join`
function from the recently released **dplyr** package.^[The functions `merge`
from the R's base package and `join` from the **plyr** provide other ways of
undertaking this step. `inner_join` is used in place of `merge` because `merge`
does not maintain row order.  `join` generates the same result, but is slower,
hence the use of `inner_join` from the recently released and powerful **dplyr**
package.]
 Assuming **dplyr** is loaded --- with `library(plyr)`
--- one can read more about join by entering `?inner_join` in R.

```{r, message=FALSE}
ind_full <- read.csv("data/SimpleWorld/ind-full.csv")
library(dplyr) # use install.packages(dplyr) if not installed
ints_df <- inner_join(ints_df, ind_full)
```

`ints_df` represents the final spatial microdataset, representing the entirety
of SimpleWorld's population of 33 (this can be confirmed with `nrow(ints_df)`).
To select individuals from one zone only is simple using R's subsetting
notation. To select all individuals generated for zone 2, for example, the
following code is used. Note that this is the same as the output generated in
Table 5 at the end of the SimpleWorld chapter --- we have successfully modelled
the inhabitants of a fictional planet, including income!

```{r}
ints_df[ints_df$zone == 2, ]
```

```{r, echo=FALSE}
# library(knitr)
# kable(ints_df[ints_df$zone == 2,], row.names = FALSE)
```

# Spatial microsimulation in the wild {#CakeMap}

By now we have developed a good understanding of what spatial microsimulation
is, its applications and how it works. We have seen something of its
underlying theory and its implementation in R. But how can
the method can be applied 'in the wild', on real datasets?

The purpose of this chapter is to answer this question
using real data to estimate cake consumption in
different parts of Leeds, UK. The example is deliberately rather absurd to make
it more memorable. The steps are presented in a generalisable way, to be
applicable to a wide range of datasets.

The input microdataset is a randomised ('jumbled') subset of the
2009 [Dental Health Survey](http://data.gov.uk/dataset/adult_dental_health_survey),
(DHS) which covers England, Northern Ireland and Wales. xx variables are available
in the DHS including xx, xx and xxx. In terms of constraint variables, we are more
limited: the Census is the only survey that provides count data at the small area
level. From this 'domain' of available input data, with the non-geographical
individual-level DHS on one side --- the *microdata* --- and the geographically
aggregated categorical count data from the census --- the *constraint tables* ---
we must first decide which variables should be used to link the two. We must
select the 'linking variables', otherwise know as constraint variables.

## Selection of constraint variables

The selection of linking variables should not be arbitrary pre-ordained
by preconceptions. The decision of which constraints to use to
allocate individuals to zones should be context dependent:
if the research is on social exclusion, for example,
many variables could potentially be of interest, ranging from car ownership
and house tenancy though to age, gender and religion.
Often constraint variables must be decided not based on what would
would be ideal, but what datasets are available. The selection criteria
will vary from one project to the next, but there are some overriding
principles that apply to most projects:

1. **More the merrier**: each additional constraint used for
will further differentiate the spatial microdata from the input microdata.
If gender is the only constraint used, for example, the spatial microdata will
simply be a repetition of the input microdata but with small differences in
the gender ratio from one zone to the next. If five constraints are used
(e.g. age, gender, car ownership, tenancy and religion), the differences
between the spatial microdata from one zone to the next will be much more
pronounced and probably useful.

2. **Relevance to the target variable**: often spatial microsimulation is
used to generate local estimates of variables about which little geographically
disaggregated information is available. Income is a common example: we have
much information about income distributions, but little information about how
average values (let alone the distribution) of income varies from one small area
to the next. In this case income is the target variable. Therefore constraints
must be selected which are closely related to income for the output to resemble
reality. This is analogous to multiple regression (which can also be used
to estimate average income at the local level), where the correct
*explanatory variables* (i.e. constraint variables in spatial microsimulation)
must be selected to effectively predict the *dependent variable*. As with
regression models, there are techniques which can be used to identify the most
suitable constraint variables for a given target variable.

```{r, echo=FALSE}
# TODO: See section x
# may need to spell-out what constraint categories are for below
```

3. **Simplicity**: this criterion to some extent contradicts the first. Sometimes
more constraints do not result in better spatial microdata and problems associated
with 'over-fitting' can emerge. Spatial microsimulation models based on many
tens of constraint categories will take longer to run and require more time
to develop and modify. In addition, the chances of an error being
introduced during every phase of the project is increased with each additional
constraint. The extent to which increasing the number of constraint categories
improves the results of spatial microsimulation, either with additional
variables or by using cross-tabulated constraints (e.g. age/sex)
instead of single-variable constraints, has yet to be explored. It is therefore
difficult to provide general rules of thumb regarding simplicity other than
'do not overcomplicate the model with excessive constraint variables and
constraints'.

To exemplify these principles, let us consider the constraint variables available
in the CakeMap datasets. Clearly only variables available both in the individual-level and 
aggreate datasets can be chosen from. Five variables assigned to each
of the 916 individuals are available from the individual-level data,
about which data is also available from the census:

- 'Car': The number of working cars in the person's household.
- 'Sex' and 'ageband4': Gender and age group, in two separate variables. 
Age is divided into 6 groups ranging from '16--24' to '65--74'.^[R tip:
This information can be seen, once the dataset is loaded, by entering
`unique(ind$ageband4)` or, to see the counts in each category,
`summary(ind$ageband4)`. Because the variable is of type `factor`,
`levels(ind$ageband4)` will also provide this information.]
- 'NSSEC': National Statistics Socio-economic Classification: an categorical
variable classifying the individual's work into one of 10 groups including '97',
which means 'no answer' (`NA`).
- 'NCakes': the target variable, reported number of times that the respondent
consumes cake each week.

All of these variables, except for 'NCakes', have a corresponding constraint
variable to be loaded for the 124 Wards that constitute the Leeds Local Authority
in the UK. In real datasets it is rarely the case that the categories of the
individual and aggregate level data match perfectly from the outset and this
is the first problem we must overcome before running a spatial microsimulation
model of cake consumption in Leeds.

The code needed to run the main part of the example is contained within
'CakeMap.R'.
Note that this script makes frequent reference to files contained
in the folder 'data/CakeMap', where input data and processing scripts
for the project are stored.

## Preparing the input data {#CakePrep}

Often spatial microsimulation is presented in a way that suggests the data
arrived in a near perfect state, ready to be inserted directly into the model.
This is rarely the case: usually, one must spend time 
loading the data into R, re-coding categorical variables and column
names, binning continuous variables and subsetting from the microdataset. In a
typical project, data preparation can take as long as the analysis
stage.  This section builds on Chapter 3 to illustrate strategies for data
cleaning on a complex project. To learn about the data cleaning steps that may
be useful to your data, we start from the beginning in this section, with a real
(anonymised) dataset that was downloaded from the internet.

The raw constraint variables for CakeMap were downloaded from
the Infuse website (http://infuse.mimas.ac.uk/).
These, logically enough, are stored in the 'cakeMap/data/' directory
as .csv files and contain the word 'raw' in the file name
to identify the original data. The file 'age-sex-raw.csv', for example is the raw
age and sex data that was downloaded. As the screenshot in
Figure 3 illustrates, these datasets are rather verbose and
require pre-processing. The resulting 'clean' constraints are saved in files such as
'con1.csv', which stands for 'constraint 1'.

![Example of raw aggregate-level input data for CakeMap aggregate data, downloaded from
http://infuse.mimas.ac.uk/.](figures/raw-data-screenshot.jpeg)

```{r, echo=FALSE}
# A little long-winded - cut down?
# stage ([Wickham, 2014](http://vita.had.co.nz/papers/tidy-data.html)).  This
# section builds on [Chapter 3](#DataPrep) to illustrate strategies for data
# ([2014b](http://vita.had.co.nz/papers/tidy-data.html)) provides a more
```

To ensure reproducibility in the process of converting the raw data into
a form ready for spatial microsimulation, all of the steps have been saved.
Take a look at the R script files 'process-age.R', 'process-nssec.R' and
'process-car.R'. The contents of these scripts should provide an insight
into methods for data preparation in R. Wickham
(2014b) provides a more
general introduction to data reformatting.
The most difficult input dataset to deal with is the age/sex constraint data.
The steps used to clean it are saved in 'process-age.R', in the `data/CakeMap/`
folder. Take a look through this
file
and try to work out what is going on: the critical stage is grouping single year
age bands into larger groups such as 16--24.

```{r, echo=FALSE}
# TODO: add more here...
```

The end result of 'process-age.R' is a 'clean' .csv file, ready to be loaded and
used as the input into our spatial microsimulation model. Note that the last
line of 'process-age.R' is `write.csv(con1, "con1.csv", row.names = F)`.  This
is the first constraint that we load into R to reweight the individual-level
data in the next section.
The outputs from these data preparation steps are named
'con1.csv' to 'con3.csv'. For simplicity, all these were merged (by
'load-all.R')
into a single dataset called 'cons.csv'. All the input data for this section are
thus loaded with only two lines of code:

```{r}
ind <- read.csv("data/CakeMap/ind.csv")
cons <- read.csv("data/CakeMap/cons.csv")
```

Take a look at these input data using the techniques learned in the previous
section. To test your understanding, try to answer the following questions:

- What are the constraint variables?
- How many individuals are in the survey microdataset?
- How many zones will we generate spatial microdata for?

For bonus points that will test your R skills as well as your practical knowledge
of spatial microsimulation, try constructing queries in R that will automatically
answer these questions.

It is vital to understand the input datasets before trying to model them, so
take some time exploring the input. Only when these
datasets make sense (a pen and paper can help here, as well as R!) is it time to
generate the spatial microdata.

## Performing IPF on CakeMap data {#CakeIPF}

The `ipfp` reweighting strategy is concise, generalisable and computationally
efficient. On a modern laptop, the `ipfp` method was found to be *almost 40
times faster* than the 'IPFinR' method (section 4.1;
Lovelace, 2014) over 20
iterations on the CakeMap data,
completing in 2 seconds instead of over 1 minute.  This is a huge
time saving!^[These tests were conducted
using the `microbenchmark()` commands found
towards the end of the 'CakeMap.R' file.
The second of these benchmarks depends on files from
`smsim-course` (Lovelace, 2014), the repository of which can be downloaded from
(https://github.com/Robinlovelace/smsim-course).

Thanks to the preparatory steps described above,
the IPF stage can be run on a single line. After the datasets are loaded in
the first half of 'CakeMap.R', the following code creates the weight matrix:

```{r, echo=FALSE}
source("R/CakeMap.R")
```

```{r, eval=FALSE}
weights <- apply(cons, 1, function(x)
  ipfp(x, ind_catt, x0, maxit = 20))
```

As with the SimpleWorld example, the correlation between the
constraint table and the aggregated results of the spatial microsimulation
can be checked to ensure that the reweighting process has worked correctly.
This demonstrates that the process has worked with an *r* value above 0.99:

```{r}
cor(as.numeric(cons), as.numeric(ind_agg))
```

## Integerisation {#CakeINT}

As before, weights of the IPF procedure are fractional, so must be *integerised*
to create whole individuals. The code presented in chapter 4 requires little
modification to do this: it is your task to convert the weight matrix generated
by the above lines of code into a spatial microdataset called, as before,
`ints_df` (hint: the `int_trs` function in 'R/functions.R' file will help).
The spatial microdata generated in 'R/CakeMapInts.R' contain the same information
as the individual-level dataset, but with the addition of the 'zone' variable,
which specifies which zone each individual inhabits.

The spatial microdata is thus *multilevel* data, operating at one level on
the level of individuals and at another at the level of zones. To generate
summary statistics about the individuals in each zone, functions must be
run on the data, one zone (group) at a time. `aggregate` provides one
way of doing this. After converting the 'NSSEC' socio-economic class variable
into a suitable numeric variable, `aggregate` can be used to identify the
variability in social class in each zone, using the `by =` argument to
specify how the results are grouped depending on the zone each individual inhabits:

```{r, echo=TRUE, message=FALSE}
source("R/CakeMapInts.R")
```

```{r, eval=FALSE}
aggregate(ints_df$NSSEC, by = list(ints_df$zone), sd,
  na.rm = TRUE)
```

```
##    Group.1        x
## 1         1 1.970570
## 2         2 2.027638
## 3         3 2.019839
```

In the above code the third argument refers to the function to be applied to the
input data. The fourth argument is simply an argument of this function, in this
case instructing the standard deviation function (`sd`) to ignore all `NA` values.
An alternative way to perform this operation, which is faster and more
concise, is using `tapply`:

```{r, eval=FALSE}
tapply(ints_df$NSSEC, ints_df$zone, sd, na.rm = TRUE)
```

Note that operations on `ints_df` can take a few seconds to complete.
This is because the object is large, taking up much RAM on
the computer. This can be seen by asking `object.size(ints_df)` or
`nrow(ints_df)`. The latter shows we have created a spatial microdataset
of 1.6 million individuals! Try comparing
this result with the size of the original survey dataset 'ind'. Keeping an eye
on such parameters will ensure that the model does not generate datasets too
large to handle.

Next we move on to a vital
consideration in spatial microsimulation models such as CakeMap: validation.

## Model checking and validation {#svalidation}

To make an analogy with food safety standards, openness about mistakes is
conducive to high standards. Transparency in model verification is desirable for
similar reasons. The two main strategies are:

1. comparing the model results with knowledge of how it *should*
perform a-priori (model checking), and
2. comparison between the model results and empirical data (validation).

Pearson's coefficient of correlation ($r$) provides a fast and simple insight
into the fit between the simulated data and the constraints. In most cases
$r$ values greater than 0.9 should be sought in spatial microsimulation and
in many cases $r$ values exceeding 0.99 are possible, even after integerisation.
As a very basic test, let's observe the correlation between the constraint table
cells and the corresponding simulated cell values:

```{r}
cor(as.numeric(cons), as.numeric(ind_agg))
```

The total absolute error (TAE) is another commonly used measure of fit
which is the total sum of differences between the two datasets.
TAE is defined by the following formula and
R function, which can be used to measure the fit between any two
vectors or matrices of equal dimensions:^[Data frames will not
work in this function and must be converted to matrices with `as.matrix`.]

$$ TAE = \sum\limits_{ij}|U_{ij} - T_{ij}| $$

```{r}
tae <- function(observed, simulated){
  obs_vec <- as.numeric(observed)
  sim_vec <- as.numeric(simulated)
  sum(abs(obs_vec - sim_vec))
}
```

Standardised absolute error (SAE) is a related measure: $SAE = TAE/P$ where $P$ is the
total population of the study area (`tae(obs, sim)/sum(con1)` in R).  While
TAE is sensitive to the number of people within the model, SAE is not.

Pearson's *r*, TAE and SAE are just the simplest of a wide variety of
*goodness of fit* measures for comparing two datasets. The differences
between different measures are quite subtle and are discussed in detail in
Voas and Williamson (2001).
It is important to note that all such measures, that compare aggregate
count datasets, are *not* sufficient to ensure that the results of
spatial microsimulation are reliable: they are methods of *internal validation*
which simply show that the individual-level dataset has been
been reweighted to fit with a handful of constraint variables: i.e. that the
process has work under on its own terms.

Beyond typos or simple conceptual errors in model code, more fundamental
questions should be asked of spatial microsimulation models. The validity of the
assumptions on which they are built and the confidence one should have in the
results are important. For this we need external datasets.  Validation is
therefore a tricky topic, something not covered here but which is discussed in
Edwards et al. (2009). For more on this and  for (an albeit unreliable)
comparison between estimated cake consumption and external income estimates.

## Visualisations {#CakeVis}

Visualisation is an important part of communicating quantitative
data, especially so when the datasets are large and complex so not
conducive to description with tables or words.

Because we have generated spatial data, it is useful to create a map of the
results, to see how it varies from place to place.  The code used to do this
found in 'CakeMapPlot.R'. A vital function within this script is the
`inner_join` function, which depends on the **dplyr** package.


```{r, eval=FALSE}
wardsF <- inner_join(wardsF, wards@data, by = "id")
```

The above line of code by default selects all the data contained in the first
object (`wardsF`) and adds to it new variables from the second object based on
the linking variable.  Also in that script file you will encounter the function
`fortify`, the purpose of which is to convert the spatial data object into a
data frame.  The final map result of `CakeMapPlot.R' is illustrated below.

```{r, fig.cap="CakeMap results: estimated average cake consumption in Leeds", fig.width=5, fig.height=4, echo=FALSE}
library(png)
library(grid)
img <- readPNG("figures/CakeMap-lores.png")
grid.raster(img)
```

## Analysis and interpretation {#CakeAnalysis}

Once a spatial microdataset has been generated that we are happy with, we will
probably want to analyse it further.  This means exploring it --- its main
features, variability and links with other datasets. To illustrate this process
we will load an additional dataset and compare it with the estimates of cake
consumption per person  generated in the previous section at the ward level.  

The hypothesis we would like to test is that cake consumption is linked to
deprivation: More deprived people will eat unheathily and cake is a relatively
cheap 'comfort food'.  Assuming our simulated data is correct ---  a
questionable assumption but lets roll with it for now --- we can explore this at
the ward level thanks to an official
[dataset](http://www.neighbourhood.statistics.gov.uk)
on modelled income from neighbourhood statistics. The following code is taken
from the 'CakeMapPlot.R' script.

Because the income dataset was produced for old ward boundaries (they were
slightly modified for the 2011 census), we cannot merge with the spatial dataset
based on the new zone codes. Instead we rely on the name of the wards. The code
below provides a snapshot of these names and demonstrates how they can be joined
using `inner_join`.

```{r, eval=FALSE}
wards@data <- join(wards@data, imd)
summary(imd$NAME %in% wards$NAME)
##       Mode   FALSE    TRUE    NA's 
##    logical      55      71       0 
```


The above code first joins the two datasets together and then checks the result
by seeing how many matches names there are.  In practice the fit between old
names and new names is quite poor: only 71 out of 124. In a proper analysis we
would have to solve this problem (e.g. via the command `pmatch`, which stands
for partial match).  For the purposes of this exercise we will simply plot
income against simulated cake consumption to gain a feeling what it tells us
about the relationship between cake consumption and wealth.

```{r, fig.cap="The relationship between cake consumption and income according to the synthetic microdata", fig.width=5, fig.height=4, echo=FALSE}
grid.raster(readPNG("figures/incomeCake.png"))
# **Scatterplot** illustrating the relationship between modelled average ward
# income and simulated number of cakes eaten per person per week.
```

The question raised by this finding is: why?  Not why is cake consumption higher
in wealthy areas (this has not been established) but: why has the model resulted
in this correlation?  To explore this question we need to go back and look at
the individual level data. The most relevant constraint variable for income was
class.  When we look at the relationship between social class and cake
consumption in the Dental Health Survey, we find that there is indeed a link:
individuals in the highest three classes (1.1, 1.2, 2) have an average cake
intake of 3.9 cakes per week whereas the three lowest classes have an average
intake of 3.7. This is a relatively modest difference but, when averaging over
large areas, it helps explain the result.

As a bonus exercise, explore the class dependence of cake consumption in the
Dental Health Survey.

\clearpage

# Spatial microdata in agent-based models {#ABM}

Spatial microsimulation can be seen as a precursor to, or early
form of, agent-based models (ABMs). Agent-based modelling depends on 1) a number
of discrete agents, 2) with different characteristics, 3) interacting. With the
spatial microsimulation model created in the previous chapters we
already have two thirds of the necessary ingredients
of an ABMs: if your aim is to use spatial microdata as an input into
agent based models, you're more than half way there!

We do not have space in this book to describe the transition from the
spatial microdata we have generated into a full ABM. Suffice to mention some of
the tools that will be useful for the job.

[NetLogo](https://ccl.northwestern.edu/netlogo/) is a
mature and widely used toolkit for agent-based models written in
Java.  The recently published **RNetLogo** package provides an interface between
R and NetLogo, allowing for model runs to be set-up and run directly from within R
[@Thiele2014].
Crucially,
this allows the outputs of agent based models to be loaded directly into
your R environment. Using R to run a separate programme may seem overly complicated
("why not simply run the model in NetLogo?" is a valid question) for
very simply models and for setting-up and testing the 
ABM it is recommended to use NetLogo directly, with its
intuitive graphical interface.

```{r,echo=FALSE}
# TODO: Batty reference below on abm design
```


For many ABM applications, however, the R interface is useful. This is because
AMB's are inherently unpredictable due to their 'bottom up' design.
Thus we generally want to study many model runs before drawing conclusions
about how the overall system operates, let alone real world implications.
Because much of the
time taken for agent based modelling is consumed on this sensitivity/parameter space
analysis, running NetLogo
from within R makes sense, as R excells in these areas whereas NetLogo
and other agent-based modelling programs do not
[@Thiele2014]. 

```{r, echo=FALSE}

# 
# Watch this space for more on this!
# 
# https://github.com/Robinlovelace/spatial-microsim-book
```

# Additional tools and techniques

This Chapter is work in progress.

```{r, echo=FALSE}
# ## R packages for spatial microsimulation
# 
# All the code chunks presented so far, with the exception of **ggplot2** commands
# run in R's **base** package. This was a deliberate choice for robustness of
# code and minimising installation dependencies over the advantages of using
# more complex packages. There are often dozens of ways of doing one thing
# in R, potentially via a handful of packages. The way that will work
# on the maximum number of machines is often preferable, unless there is a
# clear performance advantage to using additional packages. An additional reason
# to use contributed packages sparingly (excepting the `r-recommended` packages
# endorsed by the R core team --- see http://cran.r-project.org/bin/linux/debian/README
# for details) is that the behaviour of package functions may change unexpectedly,
# whereas core R functions are likely to remain stable over many decades to come.
# 
# With this caveat out of the way, let's begin our tour of the packages of most
# potential use to spatial microsimulation.
# 
# ### **plyr** and **dplyr**
# 
# ### **reshape 2**
# 
# ### **multilevel**
# 
# ### **RNetLogo**

## The Flexible Modelling Framework (FMF)

## Allocation of home-work locations

## A spatial interaction model with individual agents

## Advanced applications in agent-based modelling
```

# Appendix: Getting up-to-speed with R {#apR}

As mentioned in Chapter 1, R is a general purpose programming
language focussed on data analysis and modelling.  This small tutorial aims to
teach the basics of R, from the perspective of spatial microsimulation research.
It should also be useful to people with existing R skills, to re-affirm their
knowledge base and see how it is applicable to spatial microsimulation.

R's design is built on the idea that "everything is an object and everything
that happens is a function". It is a *vectorised*, *object orientated* and
*functional* programming language (Wickham, 2014).  This means that R
understands vector algebra, all data accessible to R resides in a number of
named objects and that a function must be used to modify any object. We will
look at each of these in some code below.

## R understands vector algebra {#vector-alg}

A vector is simply an ordered list of numbers (Beezer, 2008).
Imagine two vectors, each consisting of 3 elements:

$$a = (1,2,3); b = (9,8,6) $$

To say that R understands vector algebra is to say that it knows how to
handle vectors in the same way a mathematician does: 

$$a + b = (a_1 + b_1, a_2 + b_2, c_3 + c_3  ) = (10,10,9) $$

This may not seem remarkable, but it is. Most programming
languages are not vectorised, so they would see $a + b$ differently.
In Python, for example, this is the answer we get:^[We can
get the right answer in Python, by typing the following:
`import numpy; a=numpy.array([1,2,3]); b=numpy.array([9,8,6]); a+b`.]

```{r, engine='python', eval=FALSE}
a = [1,2,3]
b = [9,8,6]
print(a + b)
```

`## [1, 2, 3, 9, 8, 6]`

In R, the operation *just works*, intuitively:

```{r}
a <- c(1, 2, 3)
b <- c(9, 8, 6)
a + b
```

This conciseness is clearly very useful in spatial microsimulation, as numeric
variables of the same length are common (e.g. the attributes of individuals in a
zone) and can be acted on with a minimum of effort.

## R is object orientated {#R-object}

In R, everything that exists is an object with a name and a class. This is
useful, because R's functions know automatically how to behave differently on
different objects depending on their class.

To illustrate the point, let's create two objects, each with a different class
and see how the function `summarise` behaves differently, depending on the type.
This behavior is *polymorphism* (Matloff, 2011):

```{r}
# Create a character and a vector object
char_obj <- c("red", "blue", "red", "green")
num_obj <- c(1, 4, 2, 532.1)

# Summary of each object
summary(char_obj)
summary(num_obj)

# Summary of a factor object
fac_obj <- factor(char_obj)
summary(fac_obj)
```

In the example above, the output from `summary` for the numeric object `num_obj`
was very different from that of the character vector `char_obj`. Note that
although the same information was contained in `fac_obj` (a factor), the output
from `summary` changes again.

Note that objects can be called almost anything in R with the exceptions of
names beginning with a number or containing operator symbols such as `-`, `^`
and brackets. It is good practice to think about what the purpose of an object
is before naming it: using clear and concise names can save you a huge amount of
time in the long run.


## Subsetting in R {#subsetting}

R has powerful, concise and (over time) intuitive methods for taking subsets of
data. Using the SimpleWorld example we loaded in *Data preparation*,
let's explore the `ind` object in more detail, to see
how we can select the parts of an object we are most interested in. As before,
we need to load the data:

```{r}
ind <- read.csv("data/SimpleWorld/ind.csv") 
```

Now, it is easy from within R to call a single individual (e.g. individual 3)
using the square bracket notation:

```{r}
ind[3,]
```

The above example takes a subset of `ind` all elements present on the 3rd row:
for a 2 dimensional table, anything to the left of the comma refers to rows and
anything to the right refers to columns. Note that `ind[2:3,]` and
`ind[c(3,5),]` also take subsets of the `ind` object: the square brackets can
take *vector* inputs as well as single numbers.

We can also subset by columns: the second dimension. Confusingly, this can be
done in four ways, because `ind` is an R `data.frame`^[This can be ascertained
by typing `class(ind)`. It is useful to know the class of different R objects,
so make good use of the `class()` function.] and a data frame can behave
simultaneously as a list, a matrix and a data frame (only the results of the
first are shown):

```{r}
ind$age # data.frame column name notation I
# ind[, 2] # matrix notation
# ind["age"] # column name notation II
# ind[[2]] # list notation
# ind[2] # numeric data frame notation
```

It is also possible to subset cells by both rows and columns simultaneously.
Let us select query the gender of the 4th individual, as an example
(pay attention to the relative location of the comma inside the square brackets):

```{r}
ind[4, 3] # The attribute of the 4th individual in column 3
```

A commonly used trick in R that helps with the analysis of individual-level data
is to subset a data frame based on one or more of its variables. Let's subset
first all females in our dataset and then all females over 50:

```{r}
ind[ind$sex == "f", ]
ind[ind$sex == "f" & ind$age > 50, ]
```

In the above code, R uses relational operators of equality (`==`) and inequality
(`>`) which can be used in combination using the `&` symbol. This works because,
as well as integer numbers, one can also place *boolean* variables into square
brackets: `ind$sex == "f"` returns a binary vector consisting solely of `TRUE`
and `FALSE` values.^[Thus, yet another way to invoke the 2nd column of `ind` is
the following: `ind[c(F, T, F)]`! Here, `T` and `F` are shorthand for "TRUE" and
"FALSE" respectively.] 

## Further R resources {#further}

The above tutorial should provide a sufficient grounding in R for beginners to
understand the practical examples in the book.  However, R is a deep language
and there is much else to learn that will be of benefit to your modelling
skills. There are many books excellent and tutorials that teach the fundamentals
of R for a variety of applications.
The following resources, in ascending order of difficulty,
are highly recommended:

- *Introduction to visualising spatial data in R* (Lovelace and Cheshire, 2014)
provides an introductory tutorial on handling spatial data in R, including the
administrative zone data which often form the building blocks of spatial microsimulation
models in R.
- *Introduction to scientific programming and simulation using R*
(Jones et al., 2014) is an
accessible and highly practical course that will form a solid foundation
for a range of modelling applications, including spatial microsimulation.
- *An Introduction to R* (Venables et al., 2014)
is the foundational introductory R manual, written by the
software's core developers and is available online for free.
It is terse and covers some advanced topics, but
provides a useful reference on the fundamentals of R as a language.
- *Advanced R* 
(Wickham, 2014) (http://www.crcpress.com/product/isbn/9781466586963)
delves into the heart
of the R language. It contains many advanced topics, but the introductory
chapters are straightforward. Browsing some of the pages on
Advanced R's website (http://adv-r.had.co.nz/) and
trying to answer the questions that open each chapter
provides a taster of the book and an excellent
way of testing and improving one's understanding of the R language.

```{r}
# There are alternatives to R and in the next section we will consider a few of these.
```

# Glossary

This is work in progress.

```{r, echo=FALSE}
# Any words that are highlighted in the main text can go in here

# **Population base** 
# 
# This term is roughly equivalent to the 'target population',
# used by statisticians to describe the population about whom they wish to
# draw conclusions based on a 'sample population'.
# The sample population, is the group of individuals who
# we have individual-level data for.
# 
# The **population base** is the
# complete set of individuals represented by the aggregate-level data in the
# constraint tables. A common example is the variable "Hours worked":
# only people aged 16 to 74 are generally thought of as working, so, if there is
# no `NA` (no answer) category, the population base is not the same as the total
# population of an area. A common problem faced by people using spatial microsimulation
# methods is incompatibility between aggregate constraints that use different population
# bases.
```

# Bibliography
